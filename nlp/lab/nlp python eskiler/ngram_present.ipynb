{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text1 = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]\n",
    "text2 = [['I', 'am', 'Sam'], ['Sam', 'am', 'I'], ['I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'b'), ('b', 'c')]\n",
      "[('a', 'c'), ('c', 'd'), ('d', 'c'), ('c', 'e'), ('e', 'f')]\n"
     ]
    }
   ],
   "source": [
    "for i in text1:\n",
    "    text1_bi = bigrams(i)\n",
    "    print(list(text1_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'am', 'Sam')]\n",
      "[('Sam', 'am', 'I')]\n",
      "[('I', 'do', 'not'), ('do', 'not', 'like'), ('not', 'like', 'green'), ('like', 'green', 'eggs'), ('green', 'eggs', 'and'), ('eggs', 'and', 'ham')]\n"
     ]
    }
   ],
   "source": [
    "for i in text2:\n",
    "    text1_n = ngrams(i, n = 3)\n",
    "    print(list(text1_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Padding\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'I', 'am', 'Sam', '</s>', '</s>']\n",
      "[('<s>', '<s>'), ('<s>', 'I'), ('I', 'am'), ('am', 'Sam'), ('Sam', '</s>'), ('</s>', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "# The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc.\n",
    "res = list(pad_sequence(text2[0],\n",
    "                        pad_left=True, left_pad_symbol=\"<s>\",\n",
    "                        pad_right=True, right_pad_symbol=\"</s>\",\n",
    "                        n=3))\n",
    "print(res)\n",
    "print(list(ngrams(res, n=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Padding\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Sam', 'am', 'I', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<s>', 'I'),\n",
       " ('I', 'do'),\n",
       " ('do', 'not'),\n",
       " ('not', 'like'),\n",
       " ('like', 'green'),\n",
       " ('green', 'eggs'),\n",
       " ('eggs', 'and'),\n",
       " ('and', 'ham'),\n",
       " ('ham', '</s>')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1 = list(pad_both_ends(text2[1], n=2))\n",
    "print(res1)\n",
    "\n",
    "list(bigrams(pad_both_ends(text2[2], n=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%Every\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', 'Sam'),\n",
       " ('<s>', 'Sam', 'am'),\n",
       " ('<s>', 'Sam', 'am', 'I'),\n",
       " ('<s>', 'Sam', 'am', 'I', '</s>'),\n",
       " ('Sam',),\n",
       " ('Sam', 'am'),\n",
       " ('Sam', 'am', 'I'),\n",
       " ('Sam', 'am', 'I', '</s>'),\n",
       " ('am',),\n",
       " ('am', 'I'),\n",
       " ('am', 'I', '</s>'),\n",
       " ('I',),\n",
       " ('I', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_bigrams = list(pad_both_ends(text2[1], n=2))\n",
    "list(everygrams(padded_bigrams, max_len=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Sam',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'Sam',\n",
       " 'am',\n",
       " 'I',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'I',\n",
       " 'do',\n",
       " 'not',\n",
       " 'like',\n",
       " 'green',\n",
       " 'eggs',\n",
       " 'and',\n",
       " 'ham',\n",
       " '</s>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To create this vocabulary we need to pad our sentences\n",
    "# (just like for counting ngrams) and then combine the sentences\n",
    "# into one flat stream of words.\n",
    "list(flatten(pad_both_ends(sent, n=2) for sent in text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'I', 'am', 'Sam', '</s>', '<s>', 'Sam', 'am', 'I', '</s>', '<s>', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>']\n",
      "###\n",
      "[('<s>',), ('<s>', 'I'), ('I',), ('I', 'am'), ('am',), ('am', 'Sam'), ('Sam',), ('Sam', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', 'Sam'), ('Sam',), ('Sam', 'am'), ('am',), ('am', 'I'), ('I',), ('I', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', 'I'), ('I',), ('I', 'do'), ('do',), ('do', 'not'), ('not',), ('not', 'like'), ('like',), ('like', 'green'), ('green',), ('green', 'eggs'), ('eggs',), ('eggs', 'and'), ('and',), ('and', 'ham'), ('ham',), ('ham', '</s>'), ('</s>',)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train, vocab = padded_everygram_pipeline(2, text2)\n",
    "print(list(vocab))\n",
    "print(\"###\")\n",
    "for ngram_sent in train:\n",
    "    print(list(ngram_sent))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a',\n",
       "  'goal',\n",
       "  'of',\n",
       "  'statistical',\n",
       "  'language',\n",
       "  'modeling',\n",
       "  'is',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'the',\n",
       "  'joint',\n",
       "  'probability',\n",
       "  'function',\n",
       "  'of',\n",
       "  'sequences',\n",
       "  'of',\n",
       "  'words',\n",
       "  'in',\n",
       "  'a',\n",
       "  'language',\n",
       "  '.'],\n",
       " ['this',\n",
       "  'is',\n",
       "  'intrinsically',\n",
       "  'difficult',\n",
       "  'because',\n",
       "  'of',\n",
       "  'the',\n",
       "  'curse',\n",
       "  'of',\n",
       "  'dimensionality',\n",
       "  ':',\n",
       "  'a',\n",
       "  'word',\n",
       "  'sequence',\n",
       "  'on',\n",
       "  'which',\n",
       "  'the',\n",
       "  'model',\n",
       "  'will',\n",
       "  'be',\n",
       "  'tested',\n",
       "  'is',\n",
       "  'likely',\n",
       "  'to',\n",
       "  'be',\n",
       "  'different',\n",
       "  'from',\n",
       "  'all',\n",
       "  'the',\n",
       "  'word',\n",
       "  'sequences',\n",
       "  'seen',\n",
       "  'during',\n",
       "  'training',\n",
       "  '.'],\n",
       " ['traditional',\n",
       "  'but',\n",
       "  'very',\n",
       "  'successful',\n",
       "  'approaches',\n",
       "  'based',\n",
       "  'on',\n",
       "  'n-grams',\n",
       "  'obtain',\n",
       "  'generalization',\n",
       "  'by',\n",
       "  'concatenating',\n",
       "  'very',\n",
       "  'short',\n",
       "  'overlapping',\n",
       "  'sequences',\n",
       "  'seen',\n",
       "  'in',\n",
       "  'the',\n",
       "  'training',\n",
       "  'set',\n",
       "  '.'],\n",
       " ['we',\n",
       "  'propose',\n",
       "  'to',\n",
       "  'fight',\n",
       "  'the',\n",
       "  'curse',\n",
       "  'of',\n",
       "  'dimensionality',\n",
       "  'by',\n",
       "  'learning',\n",
       "  'a',\n",
       "  'distributed',\n",
       "  'representation',\n",
       "  'for',\n",
       "  'words',\n",
       "  'which',\n",
       "  'allows',\n",
       "  'each',\n",
       "  'training',\n",
       "  'sentence',\n",
       "  'to',\n",
       "  'inform',\n",
       "  'the',\n",
       "  'model',\n",
       "  'about',\n",
       "  'an',\n",
       "  'exponential',\n",
       "  'number',\n",
       "  'of',\n",
       "  'semantically',\n",
       "  'neighboring',\n",
       "  'sentences',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'model',\n",
       "  'learns',\n",
       "  'simultaneously',\n",
       "  '(',\n",
       "  '1',\n",
       "  ')',\n",
       "  'a',\n",
       "  'distributed',\n",
       "  'representation',\n",
       "  'for',\n",
       "  'each',\n",
       "  'word',\n",
       "  'along',\n",
       "  'with',\n",
       "  '(',\n",
       "  '2',\n",
       "  ')',\n",
       "  'the',\n",
       "  'probability',\n",
       "  'function',\n",
       "  'for',\n",
       "  'word',\n",
       "  'sequences',\n",
       "  ',',\n",
       "  'expressed',\n",
       "  'in',\n",
       "  'terms',\n",
       "  'of',\n",
       "  'these',\n",
       "  'representations',\n",
       "  '.'],\n",
       " ['generalization',\n",
       "  'is',\n",
       "  'obtained',\n",
       "  'because',\n",
       "  'a',\n",
       "  'sequence',\n",
       "  'of',\n",
       "  'words',\n",
       "  'that',\n",
       "  'has',\n",
       "  'never',\n",
       "  'been',\n",
       "  'seen',\n",
       "  'before',\n",
       "  'gets',\n",
       "  'high',\n",
       "  'probability',\n",
       "  'if',\n",
       "  'it',\n",
       "  'is',\n",
       "  'made',\n",
       "  'of',\n",
       "  'words',\n",
       "  'that',\n",
       "  'are',\n",
       "  'similar',\n",
       "  '(',\n",
       "  'in',\n",
       "  'the',\n",
       "  'sense',\n",
       "  'of',\n",
       "  'having',\n",
       "  'a',\n",
       "  'nearby',\n",
       "  'representation',\n",
       "  ')',\n",
       "  'to',\n",
       "  'words',\n",
       "  'forming',\n",
       "  'an',\n",
       "  'already',\n",
       "  'seen',\n",
       "  'sentence',\n",
       "  '.'],\n",
       " ['training',\n",
       "  'such',\n",
       "  'large',\n",
       "  'models',\n",
       "  '(',\n",
       "  'with',\n",
       "  'millions',\n",
       "  'of',\n",
       "  'parameters',\n",
       "  ')',\n",
       "  'within',\n",
       "  'a',\n",
       "  'reasonable',\n",
       "  'time',\n",
       "  'is',\n",
       "  'itself',\n",
       "  'a',\n",
       "  'significant',\n",
       "  'challenge',\n",
       "  '.'],\n",
       " ['we',\n",
       "  'report',\n",
       "  'on',\n",
       "  'experiments',\n",
       "  'using',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'for',\n",
       "  'the',\n",
       "  'probability',\n",
       "  'function',\n",
       "  ',',\n",
       "  'showing',\n",
       "  'on',\n",
       "  'two',\n",
       "  'text',\n",
       "  'corpora',\n",
       "  'that',\n",
       "  'the',\n",
       "  'proposed',\n",
       "  'approach',\n",
       "  'significantly',\n",
       "  'improves',\n",
       "  'on',\n",
       "  'state-of-the-art',\n",
       "  'n-gram',\n",
       "  'models',\n",
       "  ',',\n",
       "  'and',\n",
       "  'that',\n",
       "  'the',\n",
       "  'proposed',\n",
       "  'approach',\n",
       "  'allows',\n",
       "  'to',\n",
       "  'take',\n",
       "  'advantage',\n",
       "  'of',\n",
       "  'longer',\n",
       "  'contexts',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'useful',\n",
       "  'way',\n",
       "  'to',\n",
       "  'visualize',\n",
       "  'how',\n",
       "  'different',\n",
       "  'learning',\n",
       "  'algorithms',\n",
       "  'generalize',\n",
       "  ',',\n",
       "  'inspired',\n",
       "  'from',\n",
       "  'the',\n",
       "  'view',\n",
       "  'of',\n",
       "  'non-parametric',\n",
       "  'density',\n",
       "  'estimation',\n",
       "  ',',\n",
       "  'is',\n",
       "  'to',\n",
       "  'think',\n",
       "  'of',\n",
       "  'how',\n",
       "  'probability',\n",
       "  'mass',\n",
       "  'that',\n",
       "  'is',\n",
       "  'initially',\n",
       "  'concentrated',\n",
       "  'on',\n",
       "  'the',\n",
       "  'training',\n",
       "  'points',\n",
       "  '(',\n",
       "  'e.g.',\n",
       "  ',',\n",
       "  'training',\n",
       "  'sentences',\n",
       "  ')',\n",
       "  'is',\n",
       "  'distributed',\n",
       "  'in',\n",
       "  'a',\n",
       "  'larger',\n",
       "  'volume',\n",
       "  ',',\n",
       "  'usually',\n",
       "  'in',\n",
       "  'some',\n",
       "  'form',\n",
       "  'of',\n",
       "  'neighborhood',\n",
       "  'around',\n",
       "  'the',\n",
       "  'training',\n",
       "  'points',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'high',\n",
       "  'dimensions',\n",
       "  ',',\n",
       "  'it',\n",
       "  'is',\n",
       "  'crucial',\n",
       "  'to',\n",
       "  'distribute',\n",
       "  'probability',\n",
       "  'mass',\n",
       "  'where',\n",
       "  'it',\n",
       "  'matters',\n",
       "  'rather',\n",
       "  'than',\n",
       "  'uniformly',\n",
       "  'in',\n",
       "  'all',\n",
       "  'directions',\n",
       "  'around',\n",
       "  'each',\n",
       "  'training',\n",
       "  'point',\n",
       "  '.'],\n",
       " ['we',\n",
       "  'will',\n",
       "  'show',\n",
       "  'in',\n",
       "  'this',\n",
       "  'paper',\n",
       "  'that',\n",
       "  'the',\n",
       "  'way',\n",
       "  'in',\n",
       "  'which',\n",
       "  'the',\n",
       "  'approach',\n",
       "  'proposed',\n",
       "  'here',\n",
       "  'generalizes',\n",
       "  'is',\n",
       "  'fundamentally',\n",
       "  'different',\n",
       "  'from',\n",
       "  'the',\n",
       "  'way',\n",
       "  'in',\n",
       "  'which',\n",
       "  'previous',\n",
       "  'state-of-the-art',\n",
       "  'statistical',\n",
       "  'language',\n",
       "  'modeling',\n",
       "  'approaches',\n",
       "  'are',\n",
       "  'generalizing',\n",
       "  '.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An abstract and paraghraph of the introduction from \"A neural probabilistic language model\" from\n",
    "# The Journal of Machine Learning ResearchVolume 33/1/2003 pp 1137â€“1155\n",
    "text = \"\"\"A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.\n",
    "A useful way to visualize how different learning algorithms generalize, inspired from the view of non-parametric density estimation, is to think of how probability mass that is initially concentrated on the training points (e.g., training sentences) is distributed in a larger volume, usually in some form of neighborhood around the training points. In high dimensions, it is crucial to distribute probability mass where it matters rather than uniformly in all directions around each training point. We will show in this paper that the way in which the approach proposed here generalizes is fundamentally different from the way in which previous state-of-the-art statistical language modeling approaches are generalizing.\"\"\"\n",
    "#word_tokenize(sent_tokenize(sent)[0])\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent)))\n",
    "                  for sent in sent_tokenize(text)]\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Building SLM with uni, bi, tri-grams\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "from nltk.lm import MLE\n",
    "model = MLE(n)\n",
    "\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " The model application: Checking the unknown vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 177\n",
      "('corpora', 'showing', '<UNK>')\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\", len(model.vocab))\n",
    "\n",
    "# replace words not in the vocabulary with `<UNK>`\n",
    "print(model.vocab.lookup('corpora showing allow'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " The model application: Checking counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 1194 ngrams>\n",
      "Counts of 'model' =  3\n",
      "count(language|statistical) = 2\n",
      "count(modeling |statistical language) = 2\n"
     ]
    }
   ],
   "source": [
    "print(model.counts)\n",
    "\n",
    "#Checking counts in unigrams\n",
    "print(\"Counts of 'model' = \", model.counts['model'])\n",
    "\n",
    "# Checking count in bigrams e.g. the phrase \"statistical language\" (P(language|statistiacl))\n",
    "no = model.counts[['statistical']]['language']\n",
    "print(\"count(language|statistical) =\", no)\n",
    "\n",
    "# Checking count in trigrams e.g. the phrase \"statistical language modeling\" (P(modeling|statistical language))\n",
    "no = model.counts[['statistical', 'language']]['modeling']\n",
    "print(\"count(modeling |statistical language) =\", no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Model used to score how probable words are in certain contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(language) = 0.007334963325183374\n",
      "P(distributed |representation ) = 0.6666666666666666\n",
      "P(for |distributed representation ) = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no = model.score('language')\n",
    "print(\"p(language) =\", no)\n",
    "\n",
    "# P('distributed' | representation)\n",
    "no = model.score('representation', ['distributed'])\n",
    "print(\"P(distributed |representation ) =\", no)\n",
    "\n",
    "no = model.score('for', ['distributed', 'representation'])\n",
    "print(\"P(for |distributed representation ) =\", no)\n",
    "\n",
    "#Unknown words P = 0.0\n",
    "model.score(\"<UNK>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The model evaluation (probablility, log probablility,\n",
    "cross-entropy and perplexity with respect to sequences of ngrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log score for 'model' word: -7.090994532220593\n",
      "Entropy score for ('language','modeling'),('distributed', 'representation'): 0.5849625007211563\n",
      "Perplexity score for '('language','modeling'),('distributed', 'representation'): 1.5000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(\"Log score for 'model' word:\", model.logscore(\"model\"))\n",
    "print(\"Entropy score for ('language','modeling'),('distributed', 'representation'):\",model.entropy([('language','modeling'),('distributed', 'representation')]))\n",
    "print(\"Perplexity score for '('language','modeling'),('distributed', 'representation'):\",model.perplexity([('language','modeling'),('distributed', 'representation')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "ngram models can be used to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n-grams', 'obtain', 'generalization', 'by', 'concatenating', 'very', 'short', 'overlapping', 'sequences', 'seen']\n",
      "['modeling', 'approaches', 'are', 'generalizing', '.', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(10, random_seed=10)) #Provide random_seed if you want to consistently reproduce the same text\n",
    "\n",
    "# condition generation on some preceding text\n",
    "print(model.generate(7, text_seed=['language'], random_seed=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Return to more human like sentence form\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modeling approaches are generalizing . </s> </s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "detokenize(model.generate(7, text_seed=['language'], random_seed=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
